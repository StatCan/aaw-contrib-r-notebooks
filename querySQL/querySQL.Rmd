---
title: "SQL Queries with mc"
abstract: Use mc to fetch data, then load it into R
---

# SQL Queries with Minio loaded into R

Minio implements the [S3 SELECT API](https://docs.min.io/docs/minio-select-api-quickstart-guide.html). It is not effective for creating joins or other relational database tricks, but it's phenomenal at extracting exactly the data that you need, so that your queries are blazingly fast.


For reference on how to use this SQL flavour, look at

[The AWS reference](https://docs.aws.amazon.com/AmazonS3/latest/dev/s3-glacier-select-sql-reference-select.html)

*Note: Amazon S3 Select does not support whole-object compression for Parquet objects.*
[Source](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.select_object_content)

**NOTE: The examples here use JSON, but CSV is better suited to large datasets, performing 10x faster in my experiment.**

## Check what MinIO Instances are available
```{sh, engine.path = '/bin/bash'}
ls --ignore='*.*' /vault/secrets
```

## Connect to storage

```{sh, engine.path = '/bin/bash'}
# Get the credentials, use one of the instances from the step before
source /vault/secrets/minio-standard-tenant-1
mc-original config host add minio-standard $MINIO_URL $MINIO_ACCESS_KEY $MINIO_SECRET_KEY
```

# Fast SQL Extractions

Minio implements the S3 Select API, which reads a standard amount of data off of disk. This makes the queries very fast, even on large tables. Also, you can read the data straight out of a file, without creating or managing a complex database.


## Query your data with SQL (.csv.gz or .csv or .parquet)

```{sh, engine.path = '/bin/bash'}
mc-original sql --query "
    SELECT PopTotal,PopDensity FROM s3object s
    WHERE s.Location like '%Canada%'
    LIMIT 50
" minio-standard/shared/blair-drummond/sql-example/TotalPopulation.csv.gz > Query-Data.csv
```

```{r}
df <- read.csv("Query-Data.csv", header=FALSE, col.names = c("PopTotal", "PopDensity"))

head(df)
```

**This also works with `.csv` and `.parquet`.**

### With JSON output

```{sh, engine.path = '/bin/bash'}
#<!-- NOTE: This works for CSV but cannot run SQL queries on Parquet files due to security reason  -->
### With JSON output

mc-original sql --json --query "
    SELECT PopTotal,PopDensity FROM s3object s
    WHERE s.Location like '%Canada%'
    LIMIT 50" minio-standard/shared/blair-drummond/sql-example/TotalPopulation.csv.gz > Query-Data.json
```


```{r}
library(jsonlite)
library(dplyr)

### I don't know R well, so this is magic to me, but this
### is just streaming in the file line by line to a dataframe.
### Since each line is a json string, you can't just read the whole
### file as a json


df <- "Query-Data.json"  %>% file %>% stream_in

head(df)
```

